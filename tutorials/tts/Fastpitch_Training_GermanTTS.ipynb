{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da85fd93",
   "metadata": {},
   "source": [
    "# FastPitch Training on German dataset (Characters and phonemes)\n",
    "\n",
    "This notebook is designed to provide a guide on how to train FastPitch on German dataset from scratch as part of the TTS pipeline. It contains the following sections:\n",
    "  1. **Introduction**: FastPitch and HiFi-GAN in NeMo\n",
    "  2. **Preprocessing**: How to prepare German dataset for FastPitch\n",
    "  3. **Training**: Example of FastPitch training and evaluation\n",
    "  4. **Finetuning HiFi-GAN**: Improving speech quality by Finetuning HiFi-GAN on synthesized mel-spectrograms from Fastpitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d10d7",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2022 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    "> \n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5388786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c484c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a0aa3",
   "metadata": {},
   "source": [
    "### FastPitch\n",
    "\n",
    "FastPitch is non-autoregressive model for mel-spectrogram generation based on FastSpeech, conditioned on fundamental frequency contours. For more details about model, please refer to the original [paper](https://arxiv.org/abs/2006.06873). NeMo re-implementation of FastPitch additionally uses unsupervised speech-text [aligner](https://arxiv.org/abs/2108.10447) which was originally implemented in [FastPitch 1.1](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/FastPitch).\n",
    "\n",
    "### HiFiGAN\n",
    "HiFiGAN is a generative adversarial network (GAN) model that generates audio from mel spectrograms. The generator uses transposed convolutions to upsample mel spectrograms to audio. For more details about the model, please refer to the original [paper](https://arxiv.org/abs/2010.05646). NeMo re-implementation of HiFi-GAN can be found [here](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/models/hifigan.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models.base import SpectrogramGenerator, Vocoder\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71837aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what pretrained models are available for FastPitch and Mixer-TTS\n",
    "print(\"FastPitch pretrained models:\")\n",
    "print(FastPitchModel.list_available_models())\n",
    "print(\"=====================================\")\n",
    "print(\"HiFi-GAN pretrained models:\")\n",
    "print(HifiGanModel.list_available_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc24be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load the pre-trained Fastpitch model as follows\n",
    "pretrained_model = \"tts_en_fastpitch\"\n",
    "spec_gen = FastPitchModel.from_pretrained(pretrained_model)\n",
    "spec_gen.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800eb831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In the same way, we can load the pre-trained HiFi-GAN model as follows\n",
    "pretrained_model = \"tts_hifigan\"\n",
    "vocoder = HifiGanModel.from_pretrained(pretrained_model)\n",
    "vocoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10127244",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(spec_gen, SpectrogramGenerator)\n",
    "assert isinstance(vocoder, Vocoder)\n",
    "\n",
    "with torch.no_grad():\n",
    "    parsed = spec_gen.parse(\"Hey, this produces speech!\", normalize=True)\n",
    "    spectrogram = spec_gen.generate_spectrogram(tokens=parsed)\n",
    "    audio = vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "# Now we can visualize the generated spectrogram\n",
    "# If we want to generate speech, we have to use a vocoder in conjunction to a spectrogram generator.\n",
    "# Refer to the Inference_ModelSelect notebook on how to convert spectrograms to speech.\n",
    "imshow(spectrogram.cpu().detach().numpy()[0,...], origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audio.to('cpu').numpy()[0]\n",
    "audio = audio / np.abs(audio).max()\n",
    "ipd.display(ipd.Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60a7e9",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10efcc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf7942",
   "metadata": {},
   "source": [
    "We will show example of preprocessing and training using OpenSLR's German Neutral TTS dataset ([link](https://www.openslr.org/95)). It is a free single german speaker dataset (> 23 hours) by Thorsten MÃ¼ller (voice) and Dominik Kreutz (audio optimization) for tts training. \n",
    "\n",
    "In this section, we will cover:\n",
    "1. Downloading the dataset\n",
    "2. Creating Manifests & Normalizing Text\n",
    "3. Augmenting dataset with phonemes\n",
    "4. Create dataset config\n",
    "5. Create suppplementary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a042e1",
   "metadata": {},
   "source": [
    "## 1. Downloading the dataset \n",
    "\n",
    "```bash\n",
    "mkdir /Data && cd /Data\n",
    "wget https://us.openslr.org/resources/95/thorsten-de_v02.tgz\n",
    "tar -zxvf thorsten-de_v02.tgz\n",
    "```\n",
    "\n",
    "`/Data` directory looks like:\n",
    "```bash\n",
    "$ ls /Data -R\n",
    "/Data:\n",
    "thorsten-de\n",
    "thorsten-de_v02.tgz\n",
    "\n",
    "/Data/thorsten-de:\n",
    "metadata.csv\n",
    "metadata_shuf.csv\n",
    "metadata_train.csv\n",
    "metadata_val.csv\n",
    "wavs\n",
    "\n",
    "/Data/thorsten-de/wavs:\n",
    "00025a6fbea659dae6ece011e749aa34.wav\n",
    "000314280388fb390b3e70b69ee53a23.wav\n",
    "000624f768d7e282534a850980619fb2.wav\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7503f75f",
   "metadata": {},
   "source": [
    "## 2. Creating Manifests & Normalizing Text\n",
    "\n",
    "`get_data.py` reads the `metadata.csv` provided with the dataset and generates 4 fields for each datapoint:\n",
    "1. `audio_filepath`: location of the wav file\n",
    "2. `duration`: duration of the wav file\n",
    "3. `text`: original text supplied by OpenSLR\n",
    "4. `normalized_text`: normalized text via NeMo's text normalizer \n",
    "    ```python\n",
    "    nemo_text_processing.text_normalization.normalize.Normalizer(lang=\"de\", input_case=\"cased\", overwrite_cache=True, cache_dir=str(file_path / \"cache_dir\"))\n",
    "    ```\n",
    "    \n",
    "Example record:\n",
    "```json\n",
    "{\"audio_filepath\": \"/github/datasets/openslr-95-german-neutral-tts/thorsten-de/wavs/e50eb02c25353f85549900d2fc1e0e32.wav\", \"duration\": 2.409977, \"text\": \"Geht die Schandtat auf sein Konto?\", \"normalized_text\": \"Geht die Schandtat auf sein Konto?\"}\n",
    "```\n",
    "After that, the script randomly splits the datapoints into 3 buckets, `train_manifest.json`, `val_manifest.json` and `test_manifest.json`.\n",
    "\n",
    "```bash\n",
    "\n",
    "cd /NeMo\n",
    "python scripts/dataset_processing/tts/openslr/get_data.py --data-root /Data/ --val-size 0.1 --test-size 0.2\n",
    "\n",
    "```\n",
    "\n",
    "In the example above, 10% datapoints go to validation set, 20% go to test set and the remaining 70% go to training set.\n",
    "\n",
    "`/Data` directory looks like:\n",
    "```bash\n",
    "$ ls /Data -R\n",
    "/Data:\n",
    "thorsten-de\n",
    "thorsten-de_v02.tgz\n",
    "\n",
    "/Data/thorsten-de:\n",
    "metadata.csv\n",
    "metadata_shuf.csv\n",
    "metadata_train.csv\n",
    "metadata_val.csv\n",
    "test_manifest.json\n",
    "train_manifest.json\n",
    "val_manifest.json\n",
    "wavs\n",
    "\n",
    "/Data/thorsten-de/wavs:\n",
    "00025a6fbea659dae6ece011e749aa34.wav\n",
    "000314280388fb390b3e70b69ee53a23.wav\n",
    "000624f768d7e282534a850980619fb2.wav\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832603c",
   "metadata": {},
   "source": [
    "## 3. Augmenting dataset with phonemes\n",
    "\n",
    "The original dataset only contains text input, however we want our model to be able to accept both text and phonemes. So, we need to convert our text into phonemes. We will be using [bootphon/phonemizer](https://github.com/bootphon/phonemizer) from GitHub to convert German text to phonemes. Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ea2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text: Geht die Schandtat auf sein Konto?\")\n",
    "print(\"phoneme: \\u0261e\\u02d0t di\\u02d0 \\u0283ant\\u0251\\u02d0t a\\u028af za\\u026an k\\u0254nto\\u02d0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb2c3c",
   "metadata": {},
   "source": [
    "\n",
    "The quickest way to get started with phonemizer is to use docker container:\n",
    "```bash\n",
    "git clone https://github.com/bootphon/phonemizer\n",
    "cd phonemizer\n",
    "docker build -t phonemizer .\n",
    "docker run --rm -d -it -p 8888:8888 -v /Data:/Data --ipc=host phonemizer /bin/bash\n",
    "docker exec -it <docker_container_id> /bin/bash\n",
    "pip install jupyterlab\n",
    "jupyter lab --ip=0.0.0.0 --allow-root --NotebookApp.token='' --notebook-dir=/ --NotebookApp.allow_origin='*' &\n",
    "```\n",
    "\n",
    "Within the container, run the following python script:\n",
    "\n",
    "```python\n",
    "from phonemizer.backend import EspeakBackend\n",
    "import json\n",
    "\n",
    "backend = EspeakBackend('de')\n",
    "\n",
    "input_manifest_filepath = \"/Data/thorsten-de/train_manifest.json\"\n",
    "output_manifest_filepath = \"/Data/thorsten-de/train_phonemes_manifest.json\"\n",
    "\n",
    "records = []\n",
    "n_text = []\n",
    "with open(input_manifest_filepath, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        d = json.loads(line)\n",
    "        records.append(d)\n",
    "        n_text.append(d['normalized_text'])\n",
    "\n",
    "phonemized = backend.phonemize(n_text)\n",
    "\n",
    "new_records = []\n",
    "for i in range(len(records)):\n",
    "    records[i][\"is_phoneme\"] = 0\n",
    "    new_records.append(records[i])\n",
    "    phoneme_record = records[i].copy()\n",
    "    phoneme_record[\"normalized_text\"] = phonemized[i]\n",
    "    phoneme_record[\"is_phoneme\"] = 1\n",
    "    new_records.append(phoneme_record)\n",
    "    \n",
    "with open(output_manifest_filepath, \"w\") as f:\n",
    "    for r in new_records:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "```\n",
    "\n",
    "To better understand the phonemize method, refer to the docs [here](https://github.com/bootphon/phonemizer/blob/master/phonemizer/backend/base.py#L137).\n",
    "\n",
    "Run the above script for train, test and val records, resulting in `train_phonemes_manifest.json`, `test_phonemes_manifest.json` and `val_phonemes_manifest.json` respectively.\n",
    "\n",
    "We are effectively doublng the size of our dataset. Each original record maps on to two records, one with original `normalized_text` field value and `is_phoneme` set to 0 and another with phonemized text and `is_phoneme` flag set to 1.\n",
    "\n",
    "Example of input record:\n",
    "```json\n",
    "{\"audio_filepath\": \"/Data/thorsten-de/wavs/e50eb02c25353f85549900d2fc1e0e32.wav\", \"duration\": 2.409977, \"text\": \"Geht die Schandtat auf sein Konto?\", \"normalized_text\": \"Geht die Schandtat auf sein Konto?\"}\n",
    "```\n",
    "And corresponding output records:\n",
    "```json\n",
    "{\"audio_filepath\": \"/Data/thorsten-de/wavs/e50eb02c25353f85549900d2fc1e0e32.wav\", \"duration\": 2.409977, \"text\": \"Geht die Schandtat auf sein Konto?\", \"normalized_text\": \"Geht die Schandtat auf sein Konto?\", \"is_phoneme\": 0}\n",
    "{\"audio_filepath\": \"/Data/thorsten-de/wavs/e50eb02c25353f85549900d2fc1e0e32.wav\", \"duration\": 2.409977, \"text\": \"Geht die Schandtat auf sein Konto?\", \"normalized_text\": \"\\u0261e\\u02d0t di\\u02d0 \\u0283ant\\u0251\\u02d0t a\\u028af za\\u026an k\\u0254nto\\u02d0 \", \"is_phoneme\": 1}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9ffa0",
   "metadata": {},
   "source": [
    "## 4. Create dataset config\n",
    "\n",
    "Most of the configuration remains the same as described in [FastPitch_MixerTTS_Training.ipynb](FastPitch_MixerTTS_Training.ipynb) except:\n",
    "1. The `text_tokenizer._target_` is set to `nemo.collections.tts.torch.tts_tokenizers.GermanCharsTokenizer` class, and pass `is_phoneme: true`, which will extend the `de_alphabet` to include IPA symbols, resulting in `abcdefghijklmnopqrstuvwxyzÃ¤Ã¶Ã¼ÃÊÊÅÉËÉÉ¾ÉÉªÃ§ÉÃ¸É¡ÅÉÃâ1QÌÉÊÃÉ¹ÃÊÎ¸Ã Ã³ÌÃ°Ã©ÉÃ¡`.\n",
    "\n",
    "2. The `text_normalizer.lang` is set to `de`\n",
    "\n",
    "Final config looks like:\n",
    "\n",
    "```yaml\n",
    "name: \"ds_for_fastpitch_align\"\n",
    "\n",
    "manifest_filepath: \"train_manifest.json\"\n",
    "sup_data_path: \"sup_data\"\n",
    "sup_data_types: [ \"align_prior_matrix\", \"pitch\" ]\n",
    "whitelist_path: \"nemo_text_processing/text_normalization/de/data/whitelist.tsv\"\n",
    "\n",
    "dataset:\n",
    "  _target_: nemo.collections.tts.torch.data.TTSDataset\n",
    "  manifest_filepath: ${manifest_filepath}\n",
    "  sample_rate: 22050\n",
    "  sup_data_path: ${sup_data_path}\n",
    "  sup_data_types: ${sup_data_types}\n",
    "  n_fft: 1024\n",
    "  win_length: 1024\n",
    "  hop_length: 256\n",
    "  window: \"hann\"\n",
    "  n_mels: 80\n",
    "  lowfreq: 0\n",
    "  highfreq: 8000\n",
    "  max_duration: null\n",
    "  min_duration: 0.1\n",
    "  ignore_file: null\n",
    "  trim: false\n",
    "  pitch_fmin: 65.40639132514966\n",
    "  pitch_fmax: 2093.004522404789\n",
    "\n",
    "  text_normalizer:\n",
    "    _target_: nemo_text_processing.text_normalization.normalize.Normalizer\n",
    "    lang: de\n",
    "    input_case: cased\n",
    "    whitelist: ${whitelist_path}\n",
    "\n",
    "  text_normalizer_call_kwargs:\n",
    "    verbose: false\n",
    "    punct_pre_process: true\n",
    "    punct_post_process: true\n",
    "\n",
    "  text_tokenizer:\n",
    "    _target_: nemo.collections.tts.torch.tts_tokenizers.GermanCharsTokenizer\n",
    "    punct: true\n",
    "    apostrophe: true\n",
    "    pad_with_space: true\n",
    "    phonemes: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e8154",
   "metadata": {},
   "source": [
    "## 5. Create Supplementary Data\n",
    "\n",
    "As mentioned in the [FastPitch_MixerTTS_Training.ipynb](FastPitch_MixerTTS_Training.ipynb) - To accelerate and stabilize our training, we also need to extract pitch for every audio, estimate pitch statistics (mean and std) and pre-calculate alignment prior matrices for alignment framework. To do this, all we need to do is iterate over our data one time, via `extract_sup_data.py` script.\n",
    "\n",
    "\n",
    "Example:\n",
    "```bash\n",
    "python scripts/dataset_processing/tts/extract_sup_data.py --config-path openslr/ds_conf --config-name ds_for_fastpitch_align.yaml manifest_filepath=/Data/thorsten-de/train_phonemes_manifest.json sup_data_path=/Data/thorsten-de/phonemes/\n",
    "```\n",
    "\n",
    "The above example gives the following result\n",
    "1. Creates two folders under `sup_data_path` - `pitch` and `align_prior_matrix`\n",
    "2. prints out `PITCH_MEAN, PITCH_STD = 132.524658203125, 37.389366149902344`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf2298",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a76bb",
   "metadata": {},
   "source": [
    "Before we train our model, let's define model config:\n",
    "\n",
    "```yaml\n",
    "# This config contains the default values for training FastPitch model with aligner using 22KHz sampling \n",
    "# rate. If you want to train model on other dataset, you can change config values according to your dataset. \n",
    "# Most dataset-specific arguments are in the head of the config file, see below.\n",
    "\n",
    "name: FastPitch\n",
    "\n",
    "train_dataset: ???\n",
    "validation_datasets: ???\n",
    "sup_data_path: ???\n",
    "sup_data_types: [ \"align_prior_matrix\", \"pitch\" ]\n",
    "\n",
    "# Default values from librosa.pyin\n",
    "pitch_fmin: 65.40639132514966\n",
    "pitch_fmax: 2093.004522404789\n",
    "\n",
    "# Stats (per frame), train (these values depend on pitch_fmin and pitch_fmax)\n",
    "# For example, on Thorsten Muller (German Neutral-TTS dataset): http://www.openslr.org/95/\n",
    "pitch_mean: 132.524658203125\n",
    "pitch_std: 37.389366149902344\n",
    "\n",
    "# Default values for dataset with sample_rate=22050\n",
    "sample_rate: 22050\n",
    "n_mel_channels: 80\n",
    "n_window_size: 1024\n",
    "n_window_stride: 256\n",
    "n_fft: 1024\n",
    "lowfreq: 0\n",
    "highfreq: null\n",
    "window: hann\n",
    "\n",
    "whitelist_path: \"nemo_text_processing/text_normalization/de/data/whitelist.tsv\"\n",
    "\n",
    "model:\n",
    "  learn_alignment: true\n",
    "  bin_loss_warmup_epochs: 100\n",
    "\n",
    "  n_speakers: 1\n",
    "  max_token_duration: 75\n",
    "  symbols_embedding_dim: 384\n",
    "  pitch_embedding_kernel_size: 3\n",
    "\n",
    "  pitch_fmin: ${pitch_fmin}\n",
    "  pitch_fmax: ${pitch_fmax}\n",
    "\n",
    "  pitch_mean: ${pitch_mean}\n",
    "  pitch_std: ${pitch_std}\n",
    "\n",
    "  sample_rate: ${sample_rate}\n",
    "  n_mel_channels: ${n_mel_channels}\n",
    "  n_window_size: ${n_window_size}\n",
    "  n_window_stride: ${n_window_stride}\n",
    "  n_fft: ${n_fft}\n",
    "  lowfreq: ${lowfreq}\n",
    "  highfreq: ${highfreq}\n",
    "  window: ${window}\n",
    "\n",
    "  text_normalizer:\n",
    "    _target_: nemo_text_processing.text_normalization.normalize.Normalizer\n",
    "    lang: de\n",
    "    input_case: cased\n",
    "    whitelist: ${whitelist_path}\n",
    "\n",
    "  text_normalizer_call_kwargs:\n",
    "    verbose: false\n",
    "    punct_pre_process: true\n",
    "    punct_post_process: true\n",
    "\n",
    "  text_tokenizer:\n",
    "    _target_: nemo.collections.tts.torch.tts_tokenizers.GermanCharsTokenizer\n",
    "    punct: true\n",
    "    apostrophe: true\n",
    "    pad_with_space: true\n",
    "    phonemes: true\n",
    "\n",
    "  train_ds:\n",
    "    dataset:\n",
    "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
    "      manifest_filepath: ${train_dataset}\n",
    "      sample_rate: ${model.sample_rate}\n",
    "      sup_data_path: ${sup_data_path}\n",
    "      sup_data_types: ${sup_data_types}\n",
    "      n_fft: ${model.n_fft}\n",
    "      win_length: ${model.n_window_size}\n",
    "      hop_length: ${model.n_window_stride}\n",
    "      window: ${model.window}\n",
    "      n_mels: ${model.n_mel_channels}\n",
    "      lowfreq: ${model.lowfreq}\n",
    "      highfreq: ${model.highfreq}\n",
    "      max_duration: 14 # change to null to include longer audios.\n",
    "      min_duration: 0.1\n",
    "      ignore_file: null\n",
    "      trim: false\n",
    "      pitch_fmin: ${model.pitch_fmin}\n",
    "      pitch_fmax: ${model.pitch_fmax}\n",
    "      pitch_norm: true\n",
    "      pitch_mean: ${model.pitch_mean}\n",
    "      pitch_std: ${model.pitch_std}\n",
    "\n",
    "    dataloader_params:\n",
    "      drop_last: false\n",
    "      shuffle: true\n",
    "      batch_size: 32\n",
    "      num_workers: 12\n",
    "\n",
    "  validation_ds:\n",
    "    dataset:\n",
    "      _target_: nemo.collections.tts.torch.data.TTSDataset\n",
    "      manifest_filepath: ${validation_datasets}\n",
    "      sample_rate: ${model.sample_rate}\n",
    "      sup_data_path: ${sup_data_path}\n",
    "      sup_data_types: ${sup_data_types}\n",
    "      n_fft: ${model.n_fft}\n",
    "      win_length: ${model.n_window_size}\n",
    "      hop_length: ${model.n_window_stride}\n",
    "      window: ${model.window}\n",
    "      n_mels: ${model.n_mel_channels}\n",
    "      lowfreq: ${model.lowfreq}\n",
    "      highfreq: ${model.highfreq}\n",
    "      max_duration: 14 # change to null to include longer audios.\n",
    "      min_duration: 0.1\n",
    "      ignore_file: null\n",
    "      trim: false\n",
    "      pitch_fmin: ${model.pitch_fmin}\n",
    "      pitch_fmax: ${model.pitch_fmax}\n",
    "      pitch_norm: true\n",
    "      pitch_mean: ${model.pitch_mean}\n",
    "      pitch_std: ${model.pitch_std}\n",
    "\n",
    "    dataloader_params:\n",
    "      drop_last: false\n",
    "      shuffle: false\n",
    "      batch_size: 32\n",
    "      num_workers: 2\n",
    "\n",
    "  preprocessor:\n",
    "    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
    "    features: ${model.n_mel_channels}\n",
    "    lowfreq: ${model.lowfreq}\n",
    "    highfreq: ${model.highfreq}\n",
    "    n_fft: ${model.n_fft}\n",
    "    n_window_size: ${model.n_window_size}\n",
    "    window_size: false\n",
    "    n_window_stride: ${model.n_window_stride}\n",
    "    window_stride: false\n",
    "    pad_to: 1\n",
    "    pad_value: 0\n",
    "    sample_rate: ${model.sample_rate}\n",
    "    window: ${model.window}\n",
    "    normalize: null\n",
    "    preemph: null\n",
    "    dither: 0.0\n",
    "    frame_splicing: 1\n",
    "    log: true\n",
    "    log_zero_guard_type: add\n",
    "    log_zero_guard_value: 1e-05\n",
    "    mag_power: 1.0\n",
    "\n",
    "  input_fft: #n_embed and padding_idx are added by the model\n",
    "    _target_: nemo.collections.tts.modules.transformer.FFTransformerEncoder\n",
    "    n_layer: 6\n",
    "    n_head: 1\n",
    "    d_model: ${model.symbols_embedding_dim}\n",
    "    d_head: 64\n",
    "    d_inner: 1536\n",
    "    kernel_size: 3\n",
    "    dropout: 0.1\n",
    "    dropatt: 0.1\n",
    "    dropemb: 0.0\n",
    "    d_embed: ${model.symbols_embedding_dim}\n",
    "\n",
    "  output_fft:\n",
    "    _target_: nemo.collections.tts.modules.transformer.FFTransformerDecoder\n",
    "    n_layer: 6\n",
    "    n_head: 1\n",
    "    d_model: ${model.symbols_embedding_dim}\n",
    "    d_head: 64\n",
    "    d_inner: 1536\n",
    "    kernel_size: 3\n",
    "    dropout: 0.1\n",
    "    dropatt: 0.1\n",
    "    dropemb: 0.0\n",
    "\n",
    "  alignment_module:\n",
    "    _target_: nemo.collections.tts.modules.aligner.AlignmentEncoder\n",
    "    n_text_channels: ${model.symbols_embedding_dim}\n",
    "\n",
    "  duration_predictor:\n",
    "    _target_: nemo.collections.tts.modules.fastpitch.TemporalPredictor\n",
    "    input_size: ${model.symbols_embedding_dim}\n",
    "    kernel_size: 3\n",
    "    filter_size: 256\n",
    "    dropout: 0.1\n",
    "    n_layers: 2\n",
    "\n",
    "  pitch_predictor:\n",
    "    _target_: nemo.collections.tts.modules.fastpitch.TemporalPredictor\n",
    "    input_size: ${model.symbols_embedding_dim}\n",
    "    kernel_size: 3\n",
    "    filter_size: 256\n",
    "    dropout: 0.1\n",
    "    n_layers: 2\n",
    "\n",
    "  optim:\n",
    "    name: adamw\n",
    "    lr: 1e-1\n",
    "    # optimizer arguments\n",
    "    betas: [0.9, 0.999]\n",
    "    weight_decay: 1e-6\n",
    "\n",
    "    sched:\n",
    "      name: NoamAnnealing\n",
    "      warmup_steps: 1000\n",
    "      last_epoch: -1\n",
    "      d_model: 1 # Disable scaling based on model dim\n",
    "\n",
    "trainer:\n",
    "  num_nodes: 1\n",
    "  devices: -1 # number of gpus\n",
    "  accelerator: gpu\n",
    "  strategy: ddp\n",
    "  precision: 16\n",
    "  max_epochs: 1000\n",
    "  accumulate_grad_batches: 1\n",
    "  gradient_clip_val: 1000.0\n",
    "  enable_checkpointing: false # Provided by exp_manager\n",
    "  logger: false # Provided by exp_manager\n",
    "  log_every_n_steps: 100\n",
    "  check_val_every_n_epoch: 5\n",
    "  benchmark: false\n",
    "\n",
    "exp_manager:\n",
    "  exp_dir: null\n",
    "  name: ${name}\n",
    "  create_tensorboard_logger: true\n",
    "  create_checkpoint_callback: true\n",
    "  checkpoint_callback_params:\n",
    "    monitor: v_loss\n",
    "  resume_if_exists: false\n",
    "  resume_ignore_no_checkpoint: false\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0ca3f",
   "metadata": {},
   "source": [
    "If you are using Weights and Biases, you may need to login first:\n",
    "```bash\n",
    "wandb login <api-key>\n",
    "```\n",
    "More details [here](https://docs.wandb.ai/ref/cli/wandb-login).\n",
    "\n",
    "Now we are ready for training our model! Let's try to train FastPitch.\n",
    "\n",
    "```bash\n",
    "python examples/tts/fastpitch.py --config-path conf/de --config-name fastpitch_align_22050 \\\n",
    "    model.train_ds.dataloader_params.batch_size=32 \\\n",
    "    model.validation_ds.dataloader_params.batch_size=32 \\\n",
    "    train_dataset=/Data/thorsten-de/train_phonemes_manifest.json \\\n",
    "    validation_datasets=/Data/thorsten-de/val_phonemes_manifest.json \\\n",
    "    sup_data_path=/Data/thorsten-de/phonemes/ \\\n",
    "    exp_manager.exp_dir=/result \\\n",
    "    pitch_mean=132.524658203125 \\\n",
    "    pitch_std=37.389366149902344 \\\n",
    "    +exp_manager.create_wandb_logger=true \\\n",
    "    +exp_manager.wandb_logger_kwargs.name=<wandb_run_name> \\\n",
    "    +exp_manager.wandb_logger_kwargs.project=<wandb_project_name>\n",
    "```\n",
    "\n",
    "Additionally, you may use the following flags:\n",
    "1. For limiting the number of GPUs: `CUDA_VISIBLE_DEVICES=0`\n",
    "2. For debugging: `HYDRA_FULL_ERROR=1`, `CUDA_LAUNCH_BLOCKING=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e604dec",
   "metadata": {},
   "source": [
    "## Evaluating Fastpitch + pretrained HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bbc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfg_ngc=\"tts_hifigan\"\n",
    "fastpitch_ckpt_path=\"<path_to_fastpitch_nemo_ckpt>\"\n",
    "test = \"Diese Musiksammlung soll die Vielfalt des Lebens widerspiegeln.\"\n",
    "test_id = \"877d9f668a877713b48735f282af62ca\"\n",
    "data_path = \"/Data/thorsten-de/wavs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b047512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spec_fastpitch_ckpt(spec_gen_model, v_model, test):\n",
    "    with torch.no_grad():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        parsed = spec_gen_model.parse(str_input=test, normalize=True)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed)\n",
    "        print(spectrogram.size())\n",
    "        audio = v_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "\n",
    "    spectrogram = spectrogram.to('cpu').numpy()[0]\n",
    "    audio = audio.to('cpu').numpy()[0]\n",
    "    audio = audio / np.abs(audio).max()\n",
    "    return audio, spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "vocoder_model = HifiGanModel.from_pretrained(hfg_ngc, strict=False).eval().cuda()\n",
    "spec_gen_model = FastPitchModel.restore_from(fastpitch_ckpt_path).eval().cuda() if \".nemo\" in fastpitch_ckpt_path else FastPitchModel.load_from_checkpoint(checkpoint_path=fastpitch_ckpt_path).eval().cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c680aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "audio, spectrogram = evaluate_spec_fastpitch_ckpt(spec_gen_model, vocoder_model, test)\n",
    "\n",
    "# visualize the spectrogram\n",
    "if spectrogram is not None:\n",
    "    imshow(spectrogram, origin=\"lower\")\n",
    "    plt.show()\n",
    "\n",
    "# audio\n",
    "print(\"original audio\")\n",
    "ipd.display(ipd.Audio(data_path+test_id+'.wav', rate=22050))\n",
    "print(\"predicted audio\")\n",
    "ipd.display(ipd.Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f0ab8",
   "metadata": {},
   "source": [
    "We see that audio quality is not as good as we expect. One of the ways mentioned in the [FastPitch_Finetuning.ipynb](FastPitch_Finetuning.ipynb) tutorial is to finetune HiFi-GAN. Lets try that out next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321761e5",
   "metadata": {},
   "source": [
    "# Finetuning HiFi-GAN\n",
    "\n",
    "Improving speech quality by Finetuning HiFi-GAN on synthesized mel-spectrograms from Fastpitch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f0f42",
   "metadata": {},
   "source": [
    "## Generate synthetic mels\n",
    "\n",
    "In order to generate synthetic mels, we take the latest checkpoint from Fastpitch training and predict spectrograms for each of the input records in `train_phonemes_manifest.json`, `test_phonemes_manifest.json` and `val_phonemes_manifest.json`.\n",
    "\n",
    "```python\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from nemo.collections.tts.torch.helpers import BetaBinomialInterpolator\n",
    "\n",
    "folder_name = \"synmels\"\n",
    "fastpitch_model_ckpt_path = \"/result/checkpoints/FastPitch--v_loss=0.6877-epoch=999-last.ckpt\"\n",
    "dataset_part = \"test_phonemes\" # or \"val_phonemes\", \"train_phonemes\"\n",
    "dataset_base_path = \"/Data/\"\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "spec_model = FastPitchModel.load_from_checkpoint(fastpitch_model_ckpt_path)\n",
    "spec_model.eval().cuda()\n",
    "    \n",
    "def load_wav(audio_file):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "    return samples.transpose()\n",
    "    \n",
    "# Get records from the training manifest\n",
    "manifest_path = dataset_base_path+\"thorsten-de/\"+dataset_part+\"_manifest.json\"\n",
    "records = []\n",
    "with open(manifest_path, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        records.append(json.loads(line))\n",
    "        p\n",
    "beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "\n",
    "spec_model.eval()\n",
    "device = spec_model.device\n",
    "\n",
    "save_dir = Path(dataset_base_path+folder_name+\"/\"+dataset_part)\n",
    "\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Generate a spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
    "for i, r in enumerate(records):\n",
    "    audio = load_wav(r[\"audio_filepath\"])\n",
    "\n",
    "    audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "    audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    # Again, our finetuned FastPitch model doesn't use multiple speakers,\n",
    "    # but we keep the code to support it here for reference\n",
    "    if spec_model.fastpitch.speaker_emb is not None and \"speaker\" in r:\n",
    "        speaker = torch.tensor([r['speaker']]).to(device)\n",
    "    else:\n",
    "        speaker = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if \"normalized_text\" in r:\n",
    "            text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
    "        else:\n",
    "            text = spec_model.parse(r['text'])\n",
    "\n",
    "        text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "\n",
    "        # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
    "        attn_prior = torch.from_numpy(\n",
    "          beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "        ).unsqueeze(0).to(text.device)\n",
    "        \n",
    "        spectrogram = spec_model.forward(\n",
    "              text=text, \n",
    "              input_lens=text_len, \n",
    "              spec=spect, \n",
    "              mel_lens=spect_len, \n",
    "              attn_prior=attn_prior,\n",
    "              speaker=speaker,\n",
    "            )[0]\n",
    "\n",
    "    save_path = save_dir / f\"mel_{i}.npy\"\n",
    "    np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "    r[\"mel_filepath\"] = str(save_path)\n",
    "\n",
    "hifigan_manifest_path = dataset_base_path+folder_name+\"/hifigan_\"+dataset_part+\"_ft.json\"\n",
    "\n",
    "with open(hifigan_manifest_path, \"w\") as f:\n",
    "    for r in records:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "```\n",
    "\n",
    "Repeat the above script for train and validation datasets as well. Finally the `/Data/synmels` will look like:\n",
    "```\n",
    "/Data/synmels/:\n",
    "hifigan_test_ft.json\n",
    "hifigan_train_ft.json\n",
    "hifigan_val_ft.json\n",
    "test\n",
    "train\n",
    "val\n",
    "\n",
    "/Data/synmels/test:\n",
    "mel_0.npy\n",
    "mel_1.npy\n",
    "...\n",
    "\n",
    "/Data/synmels/train:\n",
    "mel_0.npy\n",
    "mel_1.npy\n",
    "...\n",
    "\n",
    "/Data/synmels/val:\n",
    "mel_0.npy\n",
    "mel_1.npy\n",
    "...\n",
    "```\n",
    "\n",
    "Example hifigan manifest:\n",
    "```yaml\n",
    "{\"audio_filepath\": \"/Data/thorsten-de/wavs/e50eb02c25353f85549900d2fc1e0e32.wav\", \"duration\": 2.409977, \"text\": \"Geht die Schandtat auf sein Konto?\", \"normalized_text\": \"Geht die Schandtat auf sein Konto?\", \"mel_filepath\": \"/Data/synmels/test/mel_0.npy\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56c00b",
   "metadata": {},
   "source": [
    "## Launch finetuning:\n",
    "\n",
    "We will be re-using the existing hifigan config. and hifigan pretrained on english. \n",
    "\n",
    "```bash\n",
    "cd /Data\n",
    "wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_hifigan/versions/1.0.0rc1/zip -O tts_hifigan_1.0.0rc1.zip\n",
    "unzip tts_hifigan_1.0.0rc1.zip\n",
    "\n",
    "cd /NeMo\n",
    "python examples/tts/hifigan_finetune.py --config-path conf/hifigan --config-name hifigan.yaml \\\n",
    "    model.max_steps=100000 \\\n",
    "    model.optim.lr=0.00001 \\\n",
    "    model.train_ds=train_ds_finetune \\\n",
    "    model.validation_ds=val_ds_finetune \\\n",
    "    ~model.optim.sched \\\n",
    "    train_dataset=/Data/synmels/hifigan_train_phonemes_ft.json \\\n",
    "    validation_datasets=/Data/synmels/hifigan_val_phonemes_ft.json \\\n",
    "    exp_manager.exp_dir=/result \\\n",
    "    +init_from_nemo_model=/Data/tts_hifigan.nemo \\\n",
    "    trainer.devices=-1 \\\n",
    "    exp_manager.create_wandb_logger=true \\\n",
    "    exp_manager.wandb_logger_kwargs.name=<wandb_run_name> \\\n",
    "    exp_manager.wandb_logger_kwargs.project=<wandb_project_name>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9227665",
   "metadata": {},
   "source": [
    "## Evaluating Fastpitch and Finetuned HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfg_path = \"<path_to_finetuned_hifigan_nemo_or_ckpt>\"\n",
    "vocoder_model_pt = HifiGanModel.restore_from(hfg_path).eval().cuda() if \".nemo\" in hfg_path else HifiGanModel.load_from_checkpoint(checkpoint_path=hfg_path).eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "audio, spectrogram = evaluate_spec_fastpitch_ckpt(spec_gen_model, vocoder_model_pt, test)\n",
    "\n",
    "# visualize the spectrogram\n",
    "if spectrogram is not None:\n",
    "    imshow(spectrogram, origin=\"lower\")\n",
    "    plt.show()\n",
    "\n",
    "# audio\n",
    "print(\"original audio\")\n",
    "ipd.display(ipd.Audio(data_path+test_id+'.wav', rate=22050))\n",
    "print(\"predicted audio\")\n",
    "ipd.display(ipd.Audio(audio, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7efc0",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
